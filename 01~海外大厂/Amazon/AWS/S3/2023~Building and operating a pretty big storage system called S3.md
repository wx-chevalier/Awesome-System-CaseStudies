> [原文地址](https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html)

# Building and operating a pretty big storage system called S3

I’ve worked in computer systems software — operating systems, virtualization, storage, networks, and security — for my entire career. However, the last six years working with Amazon Simple Storage Service (S3) have forced me to think about systems in broader terms than I ever have before. In a given week, I get to be involved in everything from hard disk mechanics, firmware, and the physical properties of storage media at one end, to customer-facing performance experience and API expressiveness at the other. And the boundaries of the system are not just technical ones: I’ve had the opportunity to help engineering teams move faster, worked with finance and hardware teams to build cost-following services, and worked with customers to create gob-smackingly cool applications in areas like video streaming, genomics, and generative AI.
在我的整个职业生涯中，我一直从事计算机系统软件的工作——操作系统、虚拟化、存储、网络和安全。然而，过去六年与 Amazon Simple Storage Service （S3） 的合作迫使我比以往任何时候都更广泛地思考系统。在给定的一周内，我可以参与从硬盘机械、固件和存储介质的物理属性到面向客户的性能体验和 API 表现力的所有内容。系统的边界不仅仅是技术边界：我有机会帮助工程团队更快地行动，与财务和硬件团队合作构建成本跟踪服务，并与客户合作，在视频流、基因组学和生成人工智能等领域创建非常酷的应用程序。

What I’d really like to share with you more than anything else is my sense of wonder at the storage systems that are all collectively being built at this point in time, because they are pretty amazing. In this post, I want to cover a few of the interesting nuances of building something like S3, and the lessons learned and sometimes surprising observations from my time in S3.
我最想和大家分享的是我对此时正在共同构建的存储系统的惊奇感，因为它们非常惊人。在这篇文章中，我想介绍一些构建 S3 之类的有趣的细微差别，以及我在 S3 期间学到的经验教训，有时甚至是令人惊讶的观察结果。

## 17 years ago, on a university campus far, far away… 17 年前，在遥远的大学校园里......

[S3 launched on March 14th, 2006](https://www.allthingsdistributed.com/2006/03/s3.html), which means it turned 17 this year. It’s hard for me to wrap my head around the fact that for engineers starting their careers today, S3 has simply existed as an internet storage service for as long as you’ve been working with computers. Seventeen years ago, I was just finishing my PhD at the University of Cambridge. I was working in the lab that developed Xen, an open-source hypervisor that a few companies, including Amazon, were using to build the first public clouds. A group of us moved on from the Xen project at Cambridge to create a startup called XenSource that, instead of using Xen to build a public cloud, aimed to commercialize it by selling it as enterprise software. You might say that we missed a bit of an opportunity there. XenSource grew and was eventually acquired by Citrix, and I wound up learning a whole lot about growing teams and growing a business (and negotiating commercial leases, and fixing small server room HVAC systems, and so on) – things that I wasn’t exposed to in grad school.
S3 于 2006 年 3 月 14 日推出，这意味着它今年已经 17 岁了。我很难理解这样一个事实，即对于今天开始职业生涯的工程师来说，只要您使用计算机，S3 就只是作为一种互联网存储服务存在。十七年前，我刚刚在剑桥大学完成博士学位。我当时在开发 Xen 的实验室工作，Xen 是一个开源的虚拟机管理程序，包括亚马逊在内的一些公司正在使用它来构建第一个公共云。我们中的一群人从剑桥的 Xen 项目开始创建一个名为 XenSource 的初创公司，该公司不是使用 Xen 来构建公共云，而是旨在通过将其作为企业软件出售来将其商业化。你可能会说我们在那里错过了一点机会。XenSource 不断发展壮大，最终被 Citrix 收购，我最终学到了很多关于发展团队和发展业务（以及谈判商业租赁、修复小型服务器机房 HVAC 系统等）的知识——这是我在研究生院没有接触过的事情。

But at the time, what I was convinced I really wanted to do was to be a university professor. I applied for a bunch of faculty jobs and wound up finding one at UBC (which worked out really well, because my wife already had a job in Vancouver and we love the city). I threw myself into the faculty role and foolishly grew my lab to 18 students, which is something that I’d encourage anyone that’s starting out as an assistant professor to never, ever do. It was thrilling to have such a large lab full of amazing people and it was absolutely exhausting to try to supervise that many graduate students all at once, but, I’m pretty sure I did a horrible job of it. That said, our research lab was an incredible community of people and we built things that I’m still really proud of today, and we wrote all sorts of really fun papers on security, storage, virtualization, and networking.
但当时，我确信我真正想做的是成为一名大学教授。我申请了一堆教师工作，最后在 UBC 找到了一份（效果非常好，因为我的妻子已经在温哥华找到了一份工作，我们喜欢这个城市）。我全身心地投入到教师的角色中，愚蠢地将我的实验室发展到 18 名学生，这是我鼓励任何刚开始担任助理教授的人永远不要做的事情。拥有这样一个充满惊人人员的大型实验室是令人兴奋的，试图同时监督这么多研究生绝对是令人筋疲力尽的，但是，我很确定我做得很糟糕。也就是说，我们的研究实验室是一个令人难以置信的社区，我们构建了我今天仍然非常自豪的东西，我们写了各种关于安全、存储、虚拟化和网络的非常有趣的论文。

A little over two years into my professor job at UBC, a few of my students and I decided to do another startup. We started a company called Coho Data that took advantage of two really early technologies at the time: NVMe SSDs and programmable ethernet switches, to build a high-performance scale-out storage appliance. We grew Coho to about 150 people with offices in four countries, and once again it was an opportunity to learn things about stuff like the load bearing strength of second-floor server room floors, and analytics workflows in Wall Street hedge funds – both of which were well outside my training as a CS researcher and teacher. Coho was a wonderful and deeply educational experience, but in the end, the company didn’t work out and we had to wind it down.
在 UBC 担任教授两年多后，我和我的一些学生决定再做一家初创公司。我们创办了一家名为 Coho Data 的公司，该公司利用当时两种非常早期的技术：NVMe SSD 和可编程以太网交换机，构建了高性能横向扩展存储设备。我们将 Coho 发展到大约 150 人，在四个国家设有办事处，这又是一个学习诸如二楼服务器机房地板的承重强度以及华尔街对冲基金的分析工作流程的机会 - 这两者都远远超出了我作为 CS 研究员和教师的培训。Coho 是一次美妙而深刻的教育经历，但最终，公司没有成功，我们不得不把它放下来。

And so, I found myself sitting back in my mostly empty office at UBC. I realized that I’d graduated my last PhD student, and I wasn’t sure that I had the strength to start building a research lab from scratch all over again. I also felt like if I was going to be in a professor job where I was expected to teach students about the cloud, that I might do well to get some first-hand experience with how it actually works.
因此，我发现自己坐在 UBC 几乎空无一人的办公室里。我意识到我已经毕业了我最后一个博士生，我不确定我是否有力量从头开始建立一个研究实验室。我还觉得，如果我要从事教授的工作，在那里我应该教学生关于云的知识，我可能会很好地获得一些关于它实际工作原理的第一手经验。

I interviewed at some cloud providers, and had an especially fun time talking to the folks at Amazon and decided to join. And that’s where I work now. I’m based in Vancouver, and I’m an engineer that gets to work across all of Amazon’s storage products. So far, a whole lot of my time has been spent on S3.
我面试了一些云提供商，与亚马逊的人交谈得特别有趣，并决定加入。这就是我现在工作的地方。我住在温哥华，是一名工程师，负责亚马逊的所有存储产品。到目前为止，我的大部分时间都花在了 S3 上。

## How S3 works S3 的工作原理

When I joined Amazon in 2017, I arranged to spend most of my first day at work with Seth Markle. Seth is one of S3’s early engineers, and he took me into a little room with a whiteboard and then spent six hours explaining how S3 worked.
当我在 2017 年加入亚马逊时，我安排了第一天的大部分时间都和 Seth Markle 一起工作。Seth 是 S3 的早期工程师之一，他带我进入一个有白板的小房间，然后花了六个小时解释 S3 是如何工作的。

It was awesome. We drew pictures, and I asked question after question non-stop and I couldn’t stump Seth. It was exhausting, but in the best kind of way. Even then S3 was a very large system, but in broad strokes — which was what we started with on the whiteboard — it probably looks like most other storage systems that you’ve seen.
真棒。我们画了画，我不停地问了一个又一个问题，我不能难倒塞斯。这很累，但以最好的方式。即使在那时，S3 也是一个非常大的系统，但从广义上讲——这是我们在白板上开始使用的——它可能看起来像你见过的大多数其他存储系统。

![Whiteboard drawing of S3](https://www.allthingsdistributed.com/images/fast-s3-white-board.png)Amazon Simple Storage Service - Simple, right?
亚马逊简单存储服务 - 简单，对吧？

S3 is an object storage service with an HTTP REST API. There is a frontend fleet with a REST API, a namespace service, a storage fleet that’s full of hard disks, and a fleet that does background operations. In an enterprise context we might call these background tasks “data services,” like replication and tiering. What’s interesting here, when you look at the highest-level block diagram of S3’s technical design, is the fact that AWS tends to ship its org chart. This is a phrase that’s often used in a pretty disparaging way, but in this case it’s absolutely fascinating. Each of these broad components is a part of the S3 organization. Each has a leader, and a bunch of teams that work on it. And if we went into the next level of detail in the diagram, expanding one of these boxes out into the individual components that are inside it, what we’d find is that all the nested components are their own teams, have their own fleets, and, in many ways, operate like independent businesses.
S3 是具有 HTTP REST API 的对象存储服务。有一个带有 REST API 的前端队列、一个命名空间服务、一个充满硬盘的存储队列，以及一个执行后台操作的队列。在企业环境中，我们可以将这些后台任务称为“数据服务”，如复制和分层。当您查看 S3 技术设计的最高级别的框图时，有趣的是 AWS 倾向于发布其组织结构图。这是一个经常以相当贬低的方式使用的短语，但在这种情况下，它绝对令人着迷。这些广泛的组件中的每一个都是 S3 组织的一部分。每个人都有一个领导者，以及一群团队。如果我们进入图中的下一个细节级别，将其中一个框扩展到其中的各个组件中，我们会发现所有嵌套组件都是他们自己的团队，拥有自己的车队，并且在很多方面，像独立企业一样运作。

All in, S3 today is composed of hundreds of microservices that are structured this way. Interactions between these teams are literally API-level contracts, and, just like the code that we all write, sometimes we get modularity wrong and those team-level interactions are kind of inefficient and clunky, and it’s a bunch of work to go and fix it, but that’s part of building software, and it turns out, part of building software teams too.
总而言之，今天的 S3 由数百个以这种方式构建的微服务组成。这些团队之间的交互实际上是 API 级别的合同，就像我们都编写的代码一样，有时我们会弄错模块化，这些团队级别的交互效率低下且笨拙，需要做一堆工作来修复它，但这是构建软件的一部分，事实证明，这也是构建软件团队的一部分。

## Two early observations 两个早期观察

Before Amazon, I’d worked on research software, I’d worked on pretty widely adopted open-source software, and I’d worked on enterprise software and hardware appliances that were used in production inside some really large businesses. But by and large, that software was a thing we designed, built, tested, and shipped. It was the software that we packaged and the software that we delivered. Sure, we had escalations and support cases and we fixed bugs and shipped patches and updates, but we ultimately delivered software. Working on a global storage service like S3 was completely different: S3 is effectively a living, breathing organism. Everything, from developers writing code running next to the hard disks at the bottom of the software stack, to technicians installing new racks of storage capacity in our data centers, to customers tuning applications for performance, everything is one single, continuously evolving system. S3’s customers aren’t buying software, they are buying a service and they expect the experience of using that service to be continuously, predictably fantastic.
在亚马逊之前，我从事过研究软件的工作，我从事过相当广泛采用的开源软件，我从事过一些真正大型企业生产中使用的企业软件和硬件设备。但总的来说，该软件是我们设计、构建、测试和交付的东西。这是我们打包的软件和我们提供的软件。当然，我们有升级和支持案例，我们修复了错误并发布了补丁和更新，但我们最终交付了软件。在像 S3 这样的全球存储服务上工作是完全不同的：S3 实际上是一个活的、会呼吸的有机体。从开发人员编写在软件堆栈底部硬盘旁边运行的代码，到在我们的数据中心安装新存储容量机架的技术人员，再到调整应用程序性能的客户，一切都是一个不断发展的单一系统。S3 的客户不是在购买软件，而是在购买一项服务，他们希望使用该服务的体验能够持续、可预测地美妙。

The first observation was that **I was going to have to change, and really broaden how I thought about software systems and how they behave**. This didn’t just mean broadening thinking about software to include those hundreds of microservices that make up S3, it meant broadening to also include all the people who design, build, deploy, and operate all that code. It’s all one thing, and you can’t really think about it just as software. It’s software, hardware, and people, and it’s always growing and constantly evolving.
第一个观察结果是，我将不得不改变，并真正拓宽我对软件系统及其行为的看法。这不仅意味着扩大对软件的思考，以包括构成 S3 的数百个微服务，还意味着扩大到包括所有设计、构建、部署和操作所有这些代码的人员。这都是一回事，你不能把它当作软件来考虑。它是软件、硬件和人员，它一直在增长和不断发展。

The second observation was that despite the fact that this whiteboard diagram sketched the broad strokes of the organization and the software, it was also wildly misleading, because it completely obscured the scale of the system. Each one of the boxes represents its own collection of scaled out software services, often themselves built from collections of services. **It would literally take me years to come to terms with the scale of the system that I was working with, and even today I often find myself surprised at the consequences of that scale.**
第二个观察结果是，尽管这个白板图勾勒出了组织和软件的广泛笔触，但它也具有极大的误导性，因为它完全掩盖了系统的规模。每个框都代表其自己的横向扩展软件服务集合，这些服务通常由服务集合构建而成。实际上，我花了数年时间才能接受我正在使用的系统的规模，即使在今天，我也经常发现自己对这种规模的后果感到惊讶。

![Table of key S3 numbers as of 24-July 2023](https://www.allthingsdistributed.com/images/fast-s3-stats.png)S3 by the numbers (as of publishing this post).
S3 按数字（截至发布本文时）。

## Technical Scale: Scale and the physics of storage 技术规模：存储的规模和物理特性

It probably isn’t very surprising for me to mention that S3 is a really big system, and it is built using a LOT of hard disks. Millions of them. And if we’re talking about S3, it’s worth spending a little bit of time talking about hard drives themselves. Hard drives are amazing, and they’ve kind of always been amazing.
对我来说，提到 S3 是一个非常大的系统可能并不奇怪，它是使用大量硬盘构建的。数以百万计。如果我们谈论的是 S3，那么值得花一点时间谈论硬盘驱动器本身。硬盘很棒，而且它们一直都很棒。

The first hard drive was built by Jacob Rabinow, who was a researcher for the predecessor of the National Institute of Standards and Technology (NIST). Rabinow was an expert in magnets and mechanical engineering, and he’d been asked to build a machine to do magnetic storage on flat sheets of media, almost like pages in a book. He decided that idea was too complex and inefficient, so, stealing the idea of a spinning disk from record players, he built an array of spinning magnetic disks that could be read by a single head. To make that work, he cut a pizza slice-style notch out of each disk that the head could move through to reach the appropriate platter. Rabinow [described this](https://www.si.edu/media/NMAH/NMAH-AC0196_rabi701123.pdf) as being like “like reading a book without opening it.” The first commercially available hard disk appeared 7 years later in 1956, when IBM introduced [the 350 disk storage unit](https://en.wikipedia.org/wiki/History_of_IBM_magnetic_disk_drives#Early_IBM_HDDs), as part of the 305 RAMAC computer system. We’ll come back to the RAMAC in a bit.
第一个硬盘是由雅各布·拉比诺（Jacob Rabinow）建造的，他是美国国家标准与技术研究院（NIST）前身的研究员。Rabinow 是磁铁和机械工程方面的专家，他被要求制造一台机器，在平板介质上进行磁性存储，几乎就像书页一样。他认为这个想法太复杂和低效，所以，从唱片机中窃取了旋转磁盘的想法，他建立了一个旋转磁盘阵列，可以由一个头读取。为了做到这一点，他从每个圆盘上切出一个披萨片式的凹口，头部可以穿过这个凹口到达适当的盘子。拉比诺形容这就像“就像在不打开的情况下阅读一本书”。7 年后的 1956 年，第一个商用硬盘出现，当时 IBM 推出了 350 磁盘存储单元，作为 305 RAMAC 计算机系统的一部分。我们稍后会回到 RAMAC。

![The first magnetic memory device](https://www.allthingsdistributed.com/images/fast-magnetic-drive-chm.jpg)The first magnetic memory device. Credit: https://www.computerhistory.org/storageengine/rabinow-patents-magnetic-disk-data-storage/
第一个磁性存储设备。信用：https://www.computerhistory.org/storageengine/rabinow-patents-magnetic-disk-data-storage/

Today, 67 years after that first commercial drive was introduced, the world uses lots of hard drives. Globally, the number of bytes stored on hard disks continues to grow every year, but the applications of hard drives are clearly diminishing. We just seem to be using hard drives for fewer and fewer things. Today, consumer devices are effectively all solid-state, and a large amount of enterprise storage is similarly switching to SSDs. Jim Gray predicted this direction in 2006, when he very presciently said: “Tape is Dead. Disk is Tape. Flash is Disk. RAM Locality is King.“ This quote has been used a lot over the past couple of decades to motivate flash storage, but the thing it observes about disks is just as interesting.
今天，在推出第一个商用硬盘 67 年后，世界使用大量硬盘。在全球范围内，存储在硬盘上的字节数每年都在持续增长，但硬盘的应用显然在减少。我们似乎只是使用硬盘驱动器做越来越少的事情。如今，消费类设备实际上都是固态的，大量的企业存储也同样转向 SSD。吉姆·格雷（Jim Gray）在 2006 年预测了这个方向，当时他非常有先见之明地说：“磁带已经死了。磁盘是磁带。闪存是磁盘。RAM 地方为王。在过去的几十年里，这句话被大量用于激励闪存存储，但它观察到的关于磁盘的事情同样有趣。

Hard disks don’t fill the role of general storage media that they used to because they are big (physically and in terms of bytes), slower, and relatively fragile pieces of media. For almost every common storage application, flash is superior. But hard drives are absolute marvels of technology and innovation, and for the things they are good at, they are absolutely amazing. One of these strengths is cost efficiency, and in a large-scale system like S3, there are some unique opportunities to design around some of the constraints of individual hard disks.
硬盘不能像以前那样扮演常规存储介质的角色，因为它们很大（物理上和字节数）、速度较慢且相对脆弱的介质。对于几乎所有常见的存储应用，闪存都是优越的。但是硬盘绝对是技术和创新的奇迹，对于它们擅长的事情，它们绝对令人惊叹。这些优势之一是成本效率，在像 S3 这样的大规模系统中，有一些独特的机会可以围绕单个硬盘的一些约束进行设计。

![Diagram: The anatomy of a hard disk](https://www.allthingsdistributed.com/images/fast-hd-anatomy.jpg)The anatomy of a hard disk. Credit: https://www.researchgate.net/figure/Mechanical-components-of-a-typical-hard-disk-drive_fig8_224323123
硬盘的解剖结构。信用：https://www.researchgate.net/figure/Mechanical-components-of-a-typical-hard-disk-drive_fig8_224323123

As I was preparing for my talk at FAST, I asked Tim Rausch if he could help me revisit the old plane flying over blades of grass hard drive example. Tim did his PhD at CMU and was one of the early researchers on heat-assisted magnetic recording (HAMR) drives. Tim has worked on hard drives generally, and HAMR specifically for most of his career, and we both agreed that the plane analogy – where we scale up the head of a hard drive to be a jumbo jet and talk about the relative scale of all the other components of the drive – is a great way to illustrate the complexity and mechanical precision that’s inside an HDD. So, here’s our version for 2023.
当我准备在 FAST 的演讲时，我问 Tim Rausch 他是否可以帮我重温旧飞机飞过草叶硬盘的例子。Tim 在 CMU 获得博士学位，是热辅助磁记录（HAMR）驱动器的早期研究人员之一。Tim 通常从事硬盘工作，尤其是在他职业生涯的大部分时间里从事 HAMR 工作，我们都同意平面类比——我们将硬盘的头部放大成一架巨型喷气式飞机，并谈论硬盘所有其他组件的相对规模——是说明硬盘内部复杂性和机械精度的好方法。所以，这是我们 2023 年的版本。

Imagine a hard drive head as a 747 flying over a grassy field at 75 miles per hour. The air gap between the bottom of the plane and the top of the grass is two sheets of paper. Now, if we measure bits on the disk as blades of grass, the track width would be 4.6 blades of grass wide and the bit length would be one blade of grass. As the plane flew over the grass it would count blades of grass and only miss one blade for every 25 thousand times the plane circled the Earth.
想象一下，硬盘磁头就像一架 747 以每小时 75 英里的速度飞越草地。飞机底部和草顶之间的气隙是两张纸。现在，如果我们测量圆盘上的位作为草叶，轨道宽度将是 4.6 片草叶宽，钻头长度将是一片草叶。当飞机飞过草地时，它会数草叶，每飞机绕地球 25000 次，它只会错过一个叶片。

![img](https://www.allthingsdistributed.com/images/fast-plane-example.jpg)

That’s a bit error rate of 1 in 10^15 requests. In the real world, we see that blade of grass get missed pretty frequently – and it’s actually something we need to account for in S3.
这是 10^15 个请求中的 1 个位错误率。在现实世界中，我们看到草叶经常被错过 - 这实际上是我们需要在 S3 中考虑的事情。

Now, let’s go back to that first hard drive, the IBM RAMAC from 1956. Here are some specs on that thing:
现在，让我们回到第一个硬盘，1956 年的 IBM RAMAC。以下是该事物的一些规格：

![RAMAC hard disk stats](https://www.allthingsdistributed.com/images/fast-ramac-stats.png)

Now let’s compare it to the largest HDD that you can buy as of publishing this, which is a Western Digital Ultrastar DC HC670 26TB. Since the RAMAC, capacity has improved 7.2M times over, while the physical drive has gotten 5,000x smaller. It’s 6 billion times cheaper per byte in inflation-adjusted dollars. But despite all that, seek times – the time it takes to perform a random access to a specific piece of data on the drive – have only gotten 150x better. Why? Because they’re mechanical. We have to wait for an arm to move, for the platter to spin, and those mechanical aspects haven’t really improved at the same rate. If you are doing random reads and writes to a drive as fast as you possibly can, you can expect about 120 operations per second. The number was about the same in 2006 when S3 launched, and it was about the same even a decade before that.
现在，让我们将其与发布时可以购买的最大硬盘进行比较，即西部数据 Ultrastar DC HC670 26TB。自 RAMAC 以来，容量提高了 7.2M 倍，而物理驱动器则缩小了 5，000 倍。按通胀调整后的美元计算，每字节便宜 60 亿倍。但尽管如此，寻道时间（随机访问驱动器上特定数据所需的时间）仅提高了 150 倍。为什么？因为它们是机械的。我们必须等待手臂移动，盘片旋转，而这些机械方面并没有真正以同样的速度改善。如果您尽可能快地对驱动器进行随机读取和写入，则预计每秒大约 120 次操作。这个数字在 2006 年 S3 推出时大致相同，甚至在此之前的十年也大致相同。

This tension between HDDs growing in capacity but staying flat for performance is a central influence in S3’s design. We need to scale the number of bytes we store by moving to the largest drives we can as aggressively as we can. Today’s largest drives are 26TB, and industry roadmaps are pointing at a path to 200TB (200TB drives!) in the next decade. At that point, if we divide up our random accesses fairly across all our data, we will be allowed to do 1 I/O per second per 2TB of data on disk.
HDD 容量增长但性能保持不变之间的这种紧张关系是 S3 设计的核心影响因素。我们需要通过尽可能积极地移动到最大的驱动器来扩展我们存储的字节数。目前最大的驱动器是 26TB，行业路线图指向未来十年达到 200TB（200TB 驱动器！）的道路。此时，如果我们在所有数据中公平地划分随机访问，我们将被允许在磁盘上每 2TB 数据每秒执行 1 次 I/O。

S3 doesn’t have 200TB drives yet, but I can tell you that we anticipate using them when they’re available. And all the drive sizes between here and there.
S3 还没有 200TB 的驱动器，但我可以告诉你，我们预计在它们可用时使用它们。以及这里和那里之间的所有驱动器大小。

## Managing heat: data placement and performance 管理热量：数据放置和性能

So, with all this in mind, one of the biggest and most interesting technical scale problems that I’ve encountered is in managing and balancing I/O demand across a really large set of hard drives. In S3, we refer to that problem as heat management.
因此，考虑到所有这些，我遇到的最大和最有趣的技术规模问题之一是管理和平衡大量硬盘驱动器的 I / O 需求。在 S3 中，我们将该问题称为热管理。

By heat, I mean the number of requests that hit a given disk at any point in time. If we do a bad job of managing heat, then we end up focusing a disproportionate number of requests on a single drive, and we create hotspots because of the limited I/O that’s available from that single disk. For us, this becomes an optimization challenge of figuring out how we can place data across our disks in a way that minimizes the number of hotspots.
热量是指在任何时间点到达给定磁盘的请求数。如果我们在管理热量方面做得不好，那么我们最终会将不成比例的请求集中在单个驱动器上，并且由于单个磁盘可用的 I/O 有限，我们创建了热点。对我们来说，这变成了一个优化挑战，即弄清楚我们如何以最小化热点数量的方式将数据放置在磁盘上。

Hotspots are small numbers of overloaded drives in a system that ends up getting bogged down, and results in poor overall performance for requests dependent on those drives. When you get a hot spot, things don’t fall over, but you queue up requests and the customer experience is poor. Unbalanced load stalls requests that are waiting on busy drives, those stalls amplify up through layers of the software storage stack, they get amplified by dependent I/Os for metadata lookups or erasure coding, and they result in a very small proportion of higher latency requests — or “stragglers”. In other words, hotspots at individual hard disks create tail latency, and ultimately, if you don’t stay on top of them, they grow to eventually impact all request latency.
热点是系统中少量过载的驱动器，最终陷入困境，并导致依赖于这些驱动器的请求的整体性能不佳。当你遇到热点时，事情不会失败，但你会排队请求，客户体验很差。不平衡的负载会阻止在繁忙驱动器上等待的请求，这些停止通过软件存储堆栈的层放大，它们通过用于元数据查找或纠删码的依赖 I/O 进行放大，并且它们导致非常小比例的高延迟请求 - 或“落后者”。换句话说，单个硬盘上的热点会产生尾部延迟，最终，如果您不掌握它们，它们会增长并最终影响所有请求延迟。

As S3 scales, we want to be able to spread heat as evenly as possible, and let individual users benefit from as much of the HDD fleet as possible. This is tricky, because we don’t know when or how data is going to be accessed at the time that it’s written, and that’s when we need to decide where to place it. Before joining Amazon, I spent time doing research and building systems that tried to predict and manage this I/O heat at much smaller scales – like local hard drives or enterprise storage arrays and it was basically impossible to do a good job of. But this is a case where the sheer scale, and the multitenancy of S3 result in a system that is fundamentally different.
随着 S3 的扩展，我们希望能够尽可能均匀地传播热量，并让个人用户尽可能多地从 HDD 队列中受益。这很棘手，因为我们不知道在写入数据时何时或如何访问数据，而这时我们需要决定将其放置在哪里。在加入亚马逊之前，我花时间进行研究和构建系统，试图在更小的范围内预测和管理这种 I/O 热量——比如本地硬盘驱动器或企业存储阵列，基本上不可能做好。但在这种情况下，S3 的庞大规模和多租户导致了一个根本不同的系统。

The more workloads we run on S3, the more that individual requests to objects become decorrelated with one another. Individual storage workloads tend to be really bursty, in fact, most storage workloads are completely idle most of the time and then experience sudden load peaks when data is accessed. That peak demand is much higher than the mean. But as we aggregate millions of workloads a really, really cool thing happens: the aggregate demand smooths and it becomes way more predictable. In fact, and I found this to be a really intuitive observation once I saw it at scale, once you aggregate to a certain scale you hit a point where it is difficult or impossible for any given workload to really influence the aggregate peak at all! So, with aggregation flattening the overall demand distribution, we need to take this relatively smooth demand rate and translate it into a similarly smooth level of demand across all of our disks, balancing the heat of each workload.
我们在 S3 上运行的工作负载越多，对对象的各个请求就越多。单个存储工作负载往往非常突发，事实上，大多数存储工作负载大部分时间都处于完全空闲状态，然后在访问数据时遇到突然的负载峰值。峰值需求远高于平均水平。但是，当我们聚合数百万个工作负载时，发生了一件非常非常酷的事情：总需求变得平滑，并且变得更加可预测。事实上，一旦我大规模地看到它，我发现这是一个非常直观的观察结果，一旦你聚合到一定规模，你就达到了一个点，任何给定的工作负载都很难或不可能真正影响聚合峰值！因此，随着聚合使整体需求分布趋于平缓，我们需要将这种相对平滑的需求速率转换为所有磁盘上同样平滑的需求水平，从而平衡每个工作负载的热量。

<video autoplay="" muted="" controls="" loop="" width="100%" data-immersive-translate-effect="1" data-immersive_translate_walked="7e178460-4b01-435e-9377-c75bf80a8ca4" style="box-sizing: border-box; padding: 0px; color: rgb(53, 53, 53); font-family: -apple-system, &quot;system-ui&quot;, &quot;avenir next&quot;, avenir, helvetica, &quot;helvetica neue&quot;, ubuntu, roboto, noto, &quot;segoe ui&quot;, arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></video>

## Replication: data placement and durability 复制：数据放置和持久性

In storage systems, redundancy schemes are commonly used to protect data from hardware failures, but redundancy also helps manage heat. They spread load out and give you an opportunity to steer request traffic away from hotspots. As an example, consider replication as a simple approach to encoding and protecting data. Replication protects data if disks fail by just having multiple copies on different disks. But it also gives you the freedom to read from any of the disks. When we think about replication from a capacity perspective it’s expensive. However, from an I/O perspective – at least for reading data – replication is very efficient.
在存储系统中，冗余方案通常用于保护数据免受硬件故障的影响，但冗余也有助于管理热量。它们分散负载，让您有机会将请求流量从热点引导出去。例如，将复制视为编码和保护数据的简单方法。如果磁盘发生故障，复制只需在不同的磁盘上有多个副本即可保护数据。但它也使您可以自由地从任何磁盘读取。当我们从容量角度考虑复制时，它的成本很高。但是，从 I/O 的角度来看（至少对于读取数据而言），复制非常高效。

We obviously don’t want to pay a replication overhead for all of the data that we store, so in S3 we also make use of erasure coding. For example, we use an algorithm, such as [Reed-Solomon](https://en.wikipedia.org/wiki/Reed–Solomon_error_correction), and split our object into a set of k “identity” shards. Then we generate an additional set of m parity shards. As long as k of the (k+m) total shards remain available, we can read the object. This approach lets us reduce capacity overhead while surviving the same number of failures.
我们显然不想为我们存储的所有数据支付复制开销，因此在 S3 中我们还使用纠删码。例如，我们使用一种算法，例如 Reed-Solomon，并将我们的对象拆分为一组 k 个“身份”分片。然后我们生成一组额外的 m 奇偶校验分片。只要 （k+m） 总分片中的 k 个仍然可用，我们就可以读取对象。这种方法使我们能够减少容量开销，同时承受相同数量的故障。

## The impact of scale on data placement strategy 规模对数据放置策略的影响

So, redundancy schemes let us divide our data into more pieces than we need to read in order to access it, and that in turn provides us with the flexibility to avoid sending requests to overloaded disks, but there’s more we can do to avoid heat. The next step is to spread the placement of new objects broadly across our disk fleet. While individual objects may be encoded across tens of drives, we intentionally put different objects onto different sets of drives, so that each customer’s accesses are spread over a very large number of disks.
因此，冗余方案允许我们将数据分成比访问它所需的读取更多的部分，这反过来又为我们提供了避免向过载磁盘发送请求的灵活性，但我们可以做更多的事情来避免热量。下一步是将新对象的放置广泛分布到我们的磁盘群中。虽然单个对象可能跨数十个驱动器进行编码，但我们有意将不同的对象放在不同的驱动器集上，以便每个客户的访问都分布在大量磁盘上。

There are two big benefits to spreading the objects within each bucket across lots and lots of disks:
将每个存储桶中的对象分布在大量磁盘上有两个很大的好处：

1. A customer’s data only occupies a very small amount of any given disk, which helps achieve workload isolation, because individual workloads can’t generate a hotspot on any one disk.
   客户的数据仅占用任何给定磁盘的非常少量，这有助于实现工作负载隔离，因为单个工作负载无法在任何一个磁盘上生成热点。
2. Individual workloads can burst up to a scale of disks that would be really difficult and really expensive to build as a stand-alone system.
   单个工作负载可能会突增到一定规模的磁盘，这对于作为独立系统构建起来非常困难且成本高昂。

![A spiky workload](https://www.allthingsdistributed.com/images/fast-spiky-workload.png)Here's a spiky workload 这是一个尖峰的工作负载

For instance, look at the graph above. Think about that burst, which might be a genomics customer doing parallel analysis from thousands of Lambda functions at once. That burst of requests can be served by over a million individual disks. That’s not an exaggeration. Today, we have tens of thousands of customers with S3 buckets that are spread across millions of drives. When I first started working on S3, I was really excited (and humbled!) by the systems work to build storage at this scale, but as I really started to understand the system I realized that it was the scale of customers and workloads using the system in aggregate that really allow it to be built differently, and building at this scale means that any one of those individual workloads is able to burst to a level of performance that just wouldn’t be practical to build if they were building without this scale.
例如，看上图。想想那个突发，这可能是一个基因组学客户同时对数千个 Lambda 函数进行并行分析。这种突发请求可以由超过一百万个单独的磁盘提供服务。这并不夸张。如今，我们有数以万计的客户拥有分布在数百万个驱动器中的 S3 存储桶。当我第一次开始在 S3 上工作时，我对以这种规模构建存储的系统工作感到非常兴奋（和谦卑！），但当我真正开始了解系统时，我意识到，真正允许以不同的方式构建系统的客户和工作负载的规模。以这种规模构建意味着这些单个工作负载中的任何一个都能够突增到一个性能水平，如果他们在没有这种规模的情况下构建，则构建起来是不切实际的。

## The human factors 人为因素

Beyond the technology itself, there are human factors that make S3 - or any complex system - what it is. One of the core tenets at Amazon is that we want engineers and teams to fail fast, and safely. We want them to always have the confidence to move quickly as builders, while still remaining completely obsessed with delivering highly durable storage. One strategy we use to help with this in S3 is a process called “durability reviews.” It’s a human mechanism that’s not in the statistical 11 9s model, but it’s every bit as important.
除了技术本身之外，还有一些人为因素使 S3 - 或任何复杂的系统 - 成为它的样子。亚马逊的核心原则之一是，我们希望工程师和团队能够快速、安全地失败。我们希望他们始终有信心作为构建者快速行动，同时仍然完全痴迷于提供高度持久的存储。我们在 S3 中用于帮助解决此问题的一种策略是称为“耐久性审查”的过程。这是一种人类机制，不在统计 11 9 模型中，但它同样重要。

When an engineer makes changes that can result in a change to our durability posture, we do a durability review. The process borrows an idea from security research: the threat model. The goal is to provide a summary of the change, a comprehensive list of threats, then describe how the change is resilient to those threats. In security, writing down a threat model encourages you to think like an adversary and imagine all the nasty things that they might try to do to your system. In a durability review, we encourage the same “what are all the things that might go wrong” thinking, and really encourage engineers to be creatively critical of their own code. The process does two things very well:
当工程师所做的更改可能导致我们的耐久性状况发生变化时，我们会进行耐久性审查。这个过程借用了安全研究的一个想法：威胁模型。目标是提供更改的摘要、威胁的完整列表，然后描述更改如何对这些威胁具有弹性。在安全方面，写下威胁模型可以鼓励您像对手一样思考，并想象他们可能试图对您的系统执行的所有令人讨厌的事情。在耐久性审查中，我们鼓励同样的“所有可能出错的事情是什么”的想法，并真正鼓励工程师对自己的代码进行创造性的批评。这个过程做得很好两件事：

1. It encourages authors and reviewers to really think critically about the risks we should be protecting against.
   它鼓励作者和审稿人真正批判性地思考我们应该防范的风险。
2. It separates risk from countermeasures, and lets us have separate discussions about the two sides.
   它将风险与对策分开，让我们对双方进行单独的讨论。

When working through durability reviews we take the durability threat model, and then we evaluate whether we have the right countermeasures and protections in place. When we are identifying those protections, we really focus on identifying coarse-grained “guardrails”. These are simple mechanisms that protect you from a large class of risks. Rather than nitpicking through each risk and identifying individual mitigations, we like simple and broad strategies that protect against a lot of stuff.
在进行耐久性审查时，我们采用持久性威胁模型，然后评估我们是否有正确的对策和保护措施。当我们确定这些保护措施时，我们真正关注的是识别粗粒度的“护栏”。这些是简单的机制，可以保护您免受大量风险的影响。我们不喜欢挑剔每个风险并确定单个缓解措施，而是喜欢简单而广泛的策略来防止很多事情。

Another example of a broad strategy is demonstrated in a project we kicked off a few years back to rewrite the bottom-most layer of S3’s storage stack – the part that manages the data on each individual disk. The new storage layer is called ShardStore, and when we decided to rebuild that layer from scratch, one guardrail we put in place was to adopt a really exciting set of techniques called “lightweight formal verification”. Our team decided to shift the implementation to Rust in order to get type safety and structured language support to help identify bugs sooner, and even wrote libraries that extend that type safety to apply to on-disk structures. From a verification perspective, we built a simplified model of ShardStore’s logic, (also in Rust), and checked into the same repository alongside the real production ShardStore implementation. This model dropped all the complexity of the actual on-disk storage layers and hard drives, and instead acted as a compact but executable specification. It wound up being about 1% of the size of the real system, but allowed us to perform testing at a level that would have been completely impractical to do against a hard drive with 120 available IOPS. We even managed to [publish a paper about this work at SOSP](https://assets.amazon.science/77/5e/4a7c238f4ce890efdc325df83263/using-lightweight-formal-methods-to-validate-a-key-value-storage-node-in-amazon-s3-2.pdf).
另一个广泛策略的例子是我们几年前启动的一个项目，该项目重写了 S3 存储堆栈的最底层——管理每个磁盘上数据的部分。新的存储层称为 ShardStore，当我们决定从头开始重建该层时，我们设置的一个护栏是采用一组非常令人兴奋的技术，称为“轻量级形式验证”。我们的团队决定将实现转移到 Rust 上，以便获得类型安全和结构化语言支持，以帮助更快地识别错误，甚至编写了扩展类型安全性以应用于磁盘结构的库。从验证的角度来看，我们构建了一个简化的 ShardStore 逻辑模型（也在 Rust 中），并与实际的生产 ShardStore 实现一起签入到同一个存储库中。该模型放弃了实际磁盘存储层和硬盘驱动器的所有复杂性，而是充当紧凑但可执行的规范。它最终约为实际系统大小的 1%，但允许我们以完全不切实际的水平执行测试，这对于具有 120 可用 IOPS 的硬盘驱动器来说是完全不切实际的。我们甚至设法在 SOSP 上发表了一篇关于这项工作的论文。

From here, we’ve been able to build tools and use existing techniques, like property-based testing, to generate test cases that verify that the behaviour of the implementation matches that of the specification. The really cool bit of this work wasn’t anything to do with either designing ShardStore or using formal verification tricks. It was that we managed to kind of “industrialize” verification, taking really cool, but kind of research-y techniques for program correctness, and get them into code where normal engineers who don’t have PhDs in formal verification can contribute to maintaining the specification, and that we could continue to apply our tools with every single commit to the software. Using verification as a guardrail has given the team confidence to develop faster, and it has endured even as new engineers joined the team.
从这里开始，我们已经能够构建工具并使用现有技术（如基于属性的测试）来生成测试用例，以验证实现的行为是否与规范的行为匹配。这项工作真正酷的部分与设计 ShardStore 或使用形式验证技巧无关。而是我们设法将验证“工业化”，采用非常酷但研究的技术来提高程序的正确性，并将它们放入代码中，让没有形式验证博士学位的普通工程师可以为维护规范做出贡献，并且我们可以继续应用我们的工具，每次提交到软件。使用验证作为护栏使团队有信心更快地开发，即使新工程师加入团队，这种信心也经久不衰。

Durability reviews and lightweight formal verification are two examples of how we take a really human, and organizational view of scale in S3. The lightweight formal verification tools that we built and integrated are really technical work, **but they were motivated by a desire to let our engineers move faster and be confident even as the system becomes larger and more complex over time.** Durability reviews, similarly, are a way to help the team think about durability in a structured way, but also to make sure that we are always holding ourselves accountable for a high bar for durability as a team. There are many other examples of how we treat the organization as part of the system, and it’s been interesting to see how once you make this shift, you experiment and innovate with how the team builds and operates just as much as you do with what they are building and operating.
耐久性审查和轻量级形式验证是我们如何在 S3 中以真正人性化和组织的方式看待规模的两个示例。我们构建和集成的轻量级形式验证工具确实是技术工作，但它们的动机是希望让我们的工程师能够更快地移动并充满信心，即使系统随着时间的推移变得越来越大和复杂。同样，耐久性审查是一种帮助团队以结构化方式思考耐用性的方法，同时也确保我们始终对团队耐用性的高标准负责。还有许多其他例子说明我们如何将组织视为系统的一部分，有趣的是，一旦你做出了这种转变，你如何试验和创新团队的构建和运作方式，就像你如何构建和运营他们所做的一样。

## Scaling myself: Solving hard problems starts and ends with “Ownership” 自我扩展：解决难题始于“所有权”，终于“所有权”

The last example of scale that I’d like to tell you about is an individual one. I joined Amazon as an entrepreneur and a university professor. I’d had tens of grad students and built an engineering team of about 150 people at Coho. In the roles I’d had in the university and in startups, I loved having the opportunity to be technically creative, to build really cool systems and incredible teams, and to always be learning. But I’d never had to do that kind of role at the scale of software, people, or business that I suddenly faced at Amazon.
我想告诉你的最后一个规模的例子是一个单独的例子。我以企业家和大学教授的身份加入亚马逊。我有几十名研究生，在 Coho 建立了一个大约 150 人的工程团队。在大学和创业公司担任的角色中，我喜欢有机会在技术上发挥创造力，建立非常酷的系统和令人难以置信的团队，并始终在学习。但我从来没有像在亚马逊突然面临的软件、人员或业务规模那样做过这样的角色。

One of my favourite parts of being a CS professor was teaching the systems seminar course to graduate students. This was a course where we’d read and generally have pretty lively discussions about a collection of “classic” systems research papers. One of my favourite parts of teaching that course was that about half way through it we’d read the [SOSP Dynamo paper](https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html). I looked forward to a lot of the papers that we read in the course, but I really looked forward to the class where we read the Dynamo paper, because it was from a real production system that the students could relate to. It was Amazon, and there was a shopping cart, and that was what Dynamo was for. It’s always fun to talk about research work when people can map it to real things in their own experience.
作为一名计算机科学教授，我最喜欢的部分之一是为研究生教授系统研讨会课程。这是一门我们阅读的课程，并且通常对一系列“经典”系统研究论文进行非常热烈的讨论。在教授这门课程的过程中，我最喜欢的部分之一是，大约在一半的时候，我们阅读了 SOSP Dynamo 的论文。我期待着我们在课程中阅读的很多论文，但我真的很期待我们阅读 Dynamo 论文的课堂，因为它来自学生可以与之相关的真实生产系统。那是亚马逊，有一个购物车，这就是 Dynamo 的用途。当人们可以根据自己的经验将其映射到真实事物时，谈论研究工作总是很有趣的。

![Screenshot of the Dynamo paper](https://www.allthingsdistributed.com/images/fast-dynamo-paper-bw.jpg)

But also, technically, it was fun to discuss Dynamo, because Dynamo was [eventually consistent](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html), so it was possible for your shopping cart to be wrong.
但是，从技术上讲，讨论 Dynamo 很有趣，因为 Dynamo 最终是一致的，所以您的购物车可能会出错。

I loved this, because it was where we’d discuss what you do, practically, in production, when Dynamo was wrong. When a customer was able to place an order only to later realize that the last item had already been sold. You detected the conflict but what could you do? The customer was expecting a delivery.
我喜欢这个，因为当 Dynamo 出错时，我们会在这里讨论你在生产中实际做什么。当客户能够下订单后才意识到最后一件商品已经售出时。您检测到了冲突，但该怎么办？客户正在等待交货。

This example may have stretched the Dynamo paper’s story a little bit, but it drove to a great punchline. Because the students would often spend a bunch of discussion trying to come up with technical software solutions. Then someone would point out that this wasn’t it at all. That ultimately, these conflicts were rare, and you could resolve them by getting support staff involved and making a human decision. It was a moment where, if it worked well, you could take the class from being critical and engaged in thinking about tradeoffs and design of software systems, and you could get them to realize that the system might be bigger than that. It might be a whole organization, or a business, and maybe some of the same thinking still applied.
这个例子可能稍微拉长了迪纳摩论文的故事，但它推动了一个很棒的笑点。因为学生们经常会花一堆时间讨论，试图提出技术软件解决方案。然后有人会指出这根本不是它。最终，这些冲突很少见，您可以通过让支持人员参与并做出人为决定来解决它们。这是一个时刻，如果它运作良好，你可以把课堂从批判性转移到思考软件系统的权衡和设计中，你可以让他们意识到系统可能比这更大。它可能是整个组织，也可能是一个企业，也许一些相同的想法仍然适用。

Now that I’ve worked at Amazon for a while, I’ve come to realize that my interpretation wasn’t all that far from the truth — in terms of how the services that we run are hardly “just” the software. I’ve also realized that there’s a bit more to it than what I’d gotten out of the paper when teaching it. Amazon spends a lot of time really focused on the idea of “ownership.” The term comes up in a lot of conversations — like “does this action item have an owner?” — meaning who is the single person that is on the hook to really drive this thing to completion and make it successful.
现在我已经在亚马逊工作了一段时间，我开始意识到我的解释与事实相去甚远——就我们运行的服务如何几乎“不仅仅是”软件而言。我也意识到，它比我在教它时从论文中得到的东西要多得多。亚马逊花了很多时间真正关注“所有权”的概念。这个词出现在很多对话中——比如“这个行动项目有所有者吗？”——意思是谁是真正推动这件事完成并使其成功的一个人。

The focus on ownership actually helps understand a lot of the organizational structure and engineering approaches that exist within Amazon, and especially in S3. To move fast, to keep a really high bar for quality, teams need to be owners. They need to own the API contracts with other systems their service interacts with, they need to be completely on the hook for durability and performance and availability, and ultimately, they need to step in and fix stuff at three in the morning when an unexpected bug hurts availability. But they also need to be empowered to reflect on that bug fix and improve the system so that it doesn’t happen again. Ownership carries a lot of responsibility, but it also carries a lot of trust – because to let an individual or a team own a service, you have to give them the leeway to make their own decisions about how they are going to deliver it. It’s been a great lesson for me to realize how much allowing individuals and teams to directly own software, and more generally own a portion of the business, allows them to be passionate about what they do and really push on it. It’s also remarkable how much getting ownership wrong can have the opposite result.
对所有权的关注实际上有助于理解亚马逊内部存在的许多组织结构和工程方法，尤其是在 S3 中。为了快速行动，为了保持非常高的质量标准，团队需要成为所有者。他们需要拥有与其服务交互的其他系统的 API 合同，他们需要完全掌握持久性，性能和可用性，最终，他们需要在凌晨三点介入并修复内容，当意外错误损害可用性时。但他们也需要有能力反思错误修复并改进系统，以免再次发生。所有权承载着很多责任，但它也承载着很多信任——因为要让个人或团队拥有一项服务，你必须给他们回旋余地，让他们自己决定如何交付服务。对我来说，这是一个很好的教训，让我意识到允许个人和团队直接拥有软件，更普遍地拥有部分业务，让他们对自己的工作充满热情并真正推动它。同样值得注意的是，所有权错误会产生相反的结果。

## Encouraging ownership in others 鼓励他人拥有所有权

I’ve spent a lot of time at Amazon thinking about how important and effective the focus on ownership is to the business, but also about how effective an individual tool it is when I work with engineers and teams. I realized that the idea of recognizing and encouraging ownership had actually been a really effective tool for me in other roles. Here’s an example: In my early days as a professor at UBC, I was working with my first set of graduate students and trying to figure out how to choose great research problems for my lab. I vividly remember a conversation I had with a colleague that was also a pretty new professor at another school. When I asked them how they choose research problems with their students, they flipped. They had a surprisingly frustrated reaction. “I can’t figure this out at all. I have like 5 projects I want students to do. I’ve written them up. They hum and haw and pick one up but it never works out. I could do the projects faster myself than I can teach them to do it.”
我在亚马逊花了很多时间思考关注所有权对业务的重要性和有效性，以及当我与工程师和团队合作时，单个工具的有效性。我意识到，认可和鼓励主人翁精神的想法实际上对我来说是一个非常有效的工具。这里有一个例子：在我担任 UBC 教授的早期，我和我的第一批研究生一起工作，试图弄清楚如何为我的实验室选择伟大的研究问题。我清楚地记得我与一位同事的谈话，他也是另一所学校的新教授。当我问他们如何选择与学生一起研究问题时，他们翻脸了。他们的反应出人意料地沮丧。“我根本想不通。我有 5 个项目，我希望学生做。我已经把它们写好了。他们哼哼唧唧地捡起一个，但从来没有成功过。我自己做项目的速度比我教他们做项目的速度要快。

And ultimately, that’s actually what this person did — they were amazing, they did a bunch of really cool stuff, and wrote some great papers, and then went and joined a company and did even more cool stuff. But when I talked to grad students that worked with them what I heard was, “I just couldn’t get invested in that thing. It wasn’t my idea.”
最终，这就是这个人所做的——他们很棒，他们做了一堆非常酷的东西，写了一些很棒的论文，然后加入了一家公司，做了更酷的事情。但是当我和与他们一起工作的研究生交谈时，我听到的是，“我就是无法投资于那个东西。这不是我的主意。

As a professor, that was a pivotal moment for me. From that point forward, when I worked with students, I tried really hard to ask questions, and listen, and be excited and enthusiastic. But ultimately, my most successful research projects were never mine. They were my students and I was lucky to be involved. The thing that I don’t think I really internalized until much later, working with teams at Amazon, was that one big contribution to those projects being successful was that the students really did own them. Once students really felt like they were working on their own ideas, and that they could personally evolve it and drive it to a new result or insight, it was never difficult to get them to really invest in the work and the thinking to develop and deliver it. They just had to own it.
作为一名教授，这对我来说是一个关键时刻。从那时起，当我与学生一起工作时，我非常努力地提出问题，倾听，兴奋和热情。但最终，我最成功的研究项目从来都不是我的。他们是我的学生，我很幸运能参与其中。直到很久以后，我才真正内化，与亚马逊的团队合作，这些项目成功的一大贡献是学生们确实拥有它们。一旦学生真正觉得他们正在研究自己的想法，并且他们可以亲自发展它并将其推向新的结果或见解，让他们真正投入到工作和思维中以开发和交付它就不难了。他们只需要拥有它。

And this is probably one area of my role at Amazon that I’ve thought about and tried to develop and be more intentional about than anything else I do. As a really senior engineer in the company, of course I have strong opinions and I absolutely have a technical agenda. But If I interact with engineers by just trying to dispense ideas, it’s really hard for any of us to be successful. It’s a lot harder to get invested in an idea that you don’t own. So, when I work with teams, I’ve kind of taken the strategy that my best ideas are the ones that other people have instead of me. I consciously spend a lot more time trying to develop problems, and to do a really good job of articulating them, rather than trying to pitch solutions. **There are often multiple ways to solve a problem, and picking the right one is letting someone own the solution.** And I spend a lot of time being enthusiastic about how those solutions are developing (which is pretty easy) and encouraging folks to figure out how to have urgency and go faster (which is often a little more complex). But it has, very sincerely, been one of the most rewarding parts of my role at Amazon to approach scaling myself as an engineer being measured by making other engineers and teams successful, helping them own problems, and celebrating the wins that they achieve.
这可能是我在亚马逊工作的一个领域，我考虑并试图发展，并且比我所做的任何其他事情都更有意识。作为公司真正的高级工程师，我当然有强烈的意见，我绝对有技术议程。但是，如果我与工程师互动只是试图分配想法，我们任何人都很难成功。要投资于一个你不拥有的想法要困难得多。所以，当我与团队合作时，我采取了一种策略，即我最好的想法是其他人而不是我的想法。我有意识地花更多的时间试图提出问题，并很好地阐明它们，而不是试图提出解决方案。解决问题通常有多种方法，选择正确的方法就是让某人拥有解决方案。我花了很多时间热衷于这些解决方案的发展（这很容易），并鼓励人们弄清楚如何有紧迫感和更快（这通常有点复杂）。但是，非常真诚地，这是我在亚马逊工作中最有价值的部分之一，通过让其他工程师和团队取得成功，帮助他们解决问题，并庆祝他们取得的胜利来衡量自己作为一名工程师。

## Closing thought 结语

I came to Amazon expecting to work on a really big and complex piece of storage software. What I learned was that every aspect of my role was unbelievably bigger than that expectation. I’ve learned that the technical scale of the system is so enormous, that its workload, structure, and operations are not just bigger, but foundationally different from the smaller systems that I’d worked on in the past. I learned that it wasn’t enough to think about the software, that “the system” was also the software’s operation as a service, the organization that ran it, and the customer code that worked with it. I learned that the organization itself, as part of the system, had its own scaling challenges and provided just as many problems to solve and opportunities to innovate. And finally, I learned that to really be successful in my own role, I needed to focus on articulating the problems and not the solutions, and to find ways to support strong engineering teams in really owning those solutions.
我来到亚马逊，期望开发一个非常庞大而复杂的存储软件。我学到的是，我角色的各个方面都比预期的要大得多，令人难以置信。我了解到该系统的技术规模如此巨大，以至于它的工作负载、结构和操作不仅更大，而且与我过去使用的较小系统有着根本的不同。我了解到，仅仅考虑软件是不够的，“系统”也是软件作为服务的操作，运行它的组织以及使用它的客户代码。我了解到，作为系统的一部分，组织本身也有自己的扩展挑战，并提供同样多的问题需要解决和创新的机会。最后，我了解到，要在自己的角色中真正取得成功，我需要专注于阐明问题而不是解决方案，并找到支持强大工程团队真正拥有这些解决方案的方法。

I’m hardly done figuring any of this stuff out, but I sure feel like I’ve learned a bunch so far. Thanks for taking the time to listen.
我几乎没有弄清楚这些东西，但我确实觉得到目前为止我已经学到了很多东西。感谢您抽出宝贵时间聆听。
